# Data Pipelines: Airflow
## Project introduction

A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.

## Table of contents:

- [Objective](#objectives)
- [Dimension and fact Table](#starschema)
- [The primary scripts of the project](#scripts)
- [steps needed to run project](#runproject)
- [Datasets](#datasetsource)
- [Additional Steps that can be taken](#additionalsteps)


## Objectives
- create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills.
- Add data quality  to catch any discrepancies in the datasets.
- Process the source data in S3 to  Amazon Redshift. 

## StarSchema
- songplays: fact table - (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
- songs: dimensions table - (song_id, title, artist_id, year, duration)
- artists: dimensions table - (artist_id, name, location, lattitude, longitude)
- users: dimensions table - (user_id, first_name, last_name, gender, level)
- time : dimension table - (start_time, hour, day, week, month, year, weekday)

### Scripts
the main scripts in this project are
- sql_queries.py: It contains the SQL statements needed to create and insert the data into the tables that form our star schema
- create_tables.py: It's the script that creates our tables and drops them after use to reset the database's tables
- etl.py : It's the script that extracts all the data in json format and converts them in a dataframe that can be inserted into the tables in our star schema
- test.ipynb: to run analytic queries and assert if data was added into the tables in our star schema
- etl.ipynb: primarily used to interact with the subset of the data in the pipeline and experiment with some additional queries

### RunProject

[airflow](https://github.com/Kondwani7/Udacity-DataEngineering-Projects/tree/main/4_data_pipelines/airflow): contains the dags and operators needed to run our etl pipeline

[dags](https://github.com/Kondwani7/Udacity-DataEngineering-Projects/tree/main/4_data_pipelines/airflow/dags): within our airflow folder, it contains the dag needed to orchestrate the pipeline

[operators](https://github.com/Kondwani7/Udacity-DataEngineering-Projects/tree/main/4_data_pipelines/airflow/plugins/operators): within the plugins, it contains the custom operators built to save time writitng scripts to run our dag

## TargetPipeline

![Interface](https://github.com/Kondwani7/Udacity-DataEngineering-Projects/blob/main/4_data_pipelines/image/project_pipeline.PNG)

## DatasetSource
The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.
### Song dataset
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
###Log dataset
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The data is  sourced from S3 buckets on aws
- Song data: s3://udacity-dend/song_data
- Log data: s3://udacity-dend/log_data

## AdditionalSteps
One could adjust the scheduling times based on the internal deadline or a a third party provider's Service Level Agreement (SLA)   
