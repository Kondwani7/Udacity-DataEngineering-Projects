# Cloud Data Warehouse
## Project introduction

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Table of contents:

- [Objective](#objective)
- [Dimension and fact Table](#starschema)
- [The primary scripts of the project](#scripts)
- [steps needed to run project](#runproject)
- [Datasets](#datasetsource)
- [Additional Steps that can be taken](#additionalsteps)


## Objective
Build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights into what songs their users are listening to

## StarSchema
- songplays: fact table - (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
- songs: dimensions table - (song_id, title, artist_id, year, duration)
- artists: dimensions table - (artist_id, name, location, lattitude, longitude)
- users: dimensions table - (user_id, first_name, last_name, gender, level)
- time : dimension table - (start_time, hour, day, week, month, year, weekday)

### Scripts
the main scripts in this project are
- sql_queries.py: It contains the SQL statements needed to create and insert the data into the tables that form our star schema
- create_tables.py: It's the script that creates our tables and drops them after use to reset the database's tables
- etl.py : It's the script that extracts all the data in S3 staging buckets into redshift as SQL tables for our star schema

### RunProject
First run create_tables.py script found in the file directory
```bash
python create_tables.py
```
Go to AWS (create an account first if you don't have an account), launch a Redshift cluster, configure your security and VPC routing settings to allow access to that cluster. 

Finally, run the etl.py file in the project to finish the pipeline
```bash
python etl.py
```
## DatasetSource
### Song dataset
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
###Log dataset
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The data is  sourced from S3 buckets on aws
- Song data: s3://udacity-dend/song_data
- Log data: s3://udacity-dend/log_data

## AdditionalSteps
- One could run some analyitcal queries
- Roll up, drill down, Slice and Dice
- table joins to get key insights and summed up summaries about the data
- such as tracking a user's favorite artist summing up the with their longest listening time (duration) for an artist's song for a particular month, quarter or year
- Our for the artist, tracking the demographics of their listeners (users) such as gender
- This is the same [technqiue](https://artists.spotify.com/en/blog/how-to-read-your-spotify-for-artists-data) spotify implements to allow users & artists to track their data.

